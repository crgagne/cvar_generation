{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7b0b6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9f9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '../models/finetuned/gpt2/social_i_qa/checkpoint-1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d88362",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained(model)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model)\n",
    "#model = GPT2CustomDoubleHeadsModel.from_pretrained(args.model, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e83852",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4be5f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb127b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d15144d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"i\" in words.words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb48d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90448\n",
      "90447\n"
     ]
    }
   ],
   "source": [
    "print(words.words().index('i'))\n",
    "print(words.words().index('I'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2098eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hystrix', 'I', 'i', 'Iacchic']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.words()[90446:90450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68cb4fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'a', 'aa', 'aal', 'aalii']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.words()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1299fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = {w: 1 for w in words.words()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "701e3fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dc4e150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5968]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['hell'],\n",
    "            add_prefix_space=True).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d6566a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3607"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['Ġgives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b03133cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'Ġgives']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('He gives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ab8d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bf67ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50257/50257 [00:00<00:00, 687771.87it/s]\n"
     ]
    }
   ],
   "source": [
    "whole_words = []\n",
    "for key in tqdm(vocab.keys()):\n",
    "    key_withoutG = key.replace('Ġ','')\n",
    "    if key_withoutG in all_words.keys():\n",
    "        whole_words.append(key_withoutG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffaeae3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17359"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91665429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 't',\n",
       " 'a',\n",
       " 'he',\n",
       " 'in',\n",
       " 're',\n",
       " 'on',\n",
       " 'the',\n",
       " 'er',\n",
       " 's',\n",
       " 'at',\n",
       " 'w',\n",
       " 'o',\n",
       " 'en',\n",
       " 'c',\n",
       " 'it',\n",
       " 'is',\n",
       " 'an',\n",
       " 'or',\n",
       " 'es',\n",
       " 'b',\n",
       " 'f',\n",
       " 'ing',\n",
       " 'p',\n",
       " 'an',\n",
       " 'al',\n",
       " 'ar',\n",
       " 'to',\n",
       " 'm',\n",
       " 'of',\n",
       " 'in',\n",
       " 'd',\n",
       " 'h',\n",
       " 'and',\n",
       " 'as',\n",
       " 'th',\n",
       " 'ion',\n",
       " 'om',\n",
       " 'n',\n",
       " 'l',\n",
       " 'st',\n",
       " 're',\n",
       " 'e',\n",
       " 'ly',\n",
       " 'be',\n",
       " 'g',\n",
       " 'T',\n",
       " 'S',\n",
       " 'id',\n",
       " 'I',\n",
       " 'ut',\n",
       " 'A',\n",
       " 'is',\n",
       " 'on',\n",
       " 'am',\n",
       " 'ow',\n",
       " 'ay',\n",
       " 'ad',\n",
       " 'se',\n",
       " 'that',\n",
       " 'C',\n",
       " 'for',\n",
       " 'y',\n",
       " 'ur',\n",
       " 'u',\n",
       " 'st',\n",
       " 'M',\n",
       " 'he',\n",
       " 'it',\n",
       " 'ce',\n",
       " 'you',\n",
       " 'B',\n",
       " 'P',\n",
       " 'with',\n",
       " 'as',\n",
       " 'we',\n",
       " 'ill',\n",
       " 'D',\n",
       " 'if',\n",
       " 'ers',\n",
       " 'H',\n",
       " 'em',\n",
       " 'con',\n",
       " 'W',\n",
       " 'R',\n",
       " 'her',\n",
       " 'was',\n",
       " 'r',\n",
       " 'od',\n",
       " 'F',\n",
       " 'ate',\n",
       " 'at',\n",
       " 'ore',\n",
       " 'The',\n",
       " 'se',\n",
       " 'us',\n",
       " 'pro',\n",
       " 'ha',\n",
       " 'um',\n",
       " 'are',\n",
       " 'de',\n",
       " 'and',\n",
       " 'or',\n",
       " 'ist',\n",
       " 'N',\n",
       " 'th',\n",
       " 'G',\n",
       " 'un',\n",
       " 'L',\n",
       " 'not',\n",
       " 'ess',\n",
       " 'ex',\n",
       " 'v',\n",
       " 'E',\n",
       " 'ant',\n",
       " 'by',\n",
       " 'el',\n",
       " 'os',\n",
       " 'ort',\n",
       " 'from',\n",
       " 'have',\n",
       " 'sh',\n",
       " 'this',\n",
       " 'ra',\n",
       " 'art',\n",
       " 'al',\n",
       " 'ust',\n",
       " 'end',\n",
       " 'all',\n",
       " 'O',\n",
       " 'red',\n",
       " 'out',\n",
       " 'J',\n",
       " 'ear',\n",
       " 'ally',\n",
       " 'our',\n",
       " 'ast',\n",
       " 'can',\n",
       " 'ak',\n",
       " 'The',\n",
       " 'his',\n",
       " 'do',\n",
       " 'go',\n",
       " 'ge',\n",
       " 'U',\n",
       " 'sa',\n",
       " 'j',\n",
       " 'but',\n",
       " 'all',\n",
       " 'k',\n",
       " 'ame',\n",
       " 'will',\n",
       " 'they',\n",
       " 'ide',\n",
       " 'ich',\n",
       " 'ie',\n",
       " 'ure',\n",
       " 'age',\n",
       " 'ne',\n",
       " 'ice',\n",
       " 'me',\n",
       " 'out',\n",
       " 'one',\n",
       " 'who',\n",
       " 'K',\n",
       " 'up',\n",
       " 'their',\n",
       " 'ad',\n",
       " 'us',\n",
       " 'more',\n",
       " 'so',\n",
       " 'per',\n",
       " 'ber',\n",
       " 'act',\n",
       " 'one',\n",
       " 'said',\n",
       " 'are',\n",
       " 'your',\n",
       " 'ake',\n",
       " 'able',\n",
       " 'which',\n",
       " 'about',\n",
       " 'were',\n",
       " 'very',\n",
       " 'had',\n",
       " 'en',\n",
       " 'un',\n",
       " 'ire',\n",
       " 'ace',\n",
       " 'ary',\n",
       " 'would',\n",
       " 'ass',\n",
       " 'ere',\n",
       " 'so',\n",
       " 'V',\n",
       " 'off',\n",
       " 'te',\n",
       " 'Y',\n",
       " 'ose',\n",
       " 'man',\n",
       " 'per',\n",
       " 'other',\n",
       " 'been',\n",
       " 'like',\n",
       " 'ase',\n",
       " 'own',\n",
       " 'dis',\n",
       " 'any',\n",
       " 'ail',\n",
       " 'them',\n",
       " 'her',\n",
       " 'ar',\n",
       " 'if',\n",
       " 'there',\n",
       " 'year',\n",
       " 'my',\n",
       " 'some',\n",
       " 'when',\n",
       " 'ough',\n",
       " 'ach',\n",
       " 'than',\n",
       " 'over',\n",
       " 'ree',\n",
       " 'port',\n",
       " 'also',\n",
       " 'part',\n",
       " 'time',\n",
       " 'ens',\n",
       " 'what',\n",
       " 'no',\n",
       " 'new',\n",
       " 'get',\n",
       " 'ory',\n",
       " 'just',\n",
       " 'into',\n",
       " 'te',\n",
       " 'people',\n",
       " 'its',\n",
       " 'ark',\n",
       " 'work',\n",
       " 'ade',\n",
       " 'she',\n",
       " 'our',\n",
       " 'ink',\n",
       " 'him',\n",
       " 'ons',\n",
       " 'form',\n",
       " 'ates',\n",
       " 'only',\n",
       " 'ell',\n",
       " 'urn',\n",
       " 'how',\n",
       " 'after',\n",
       " 'aw',\n",
       " 'ne',\n",
       " 'play',\n",
       " 'could',\n",
       " 'am',\n",
       " 'first',\n",
       " 'act',\n",
       " 'hing',\n",
       " 'ull',\n",
       " 'old',\n",
       " 'fe',\n",
       " 'bet',\n",
       " 'we',\n",
       " 'two',\n",
       " 'ock',\n",
       " 'back',\n",
       " 'under',\n",
       " 'rough',\n",
       " 'may',\n",
       " 'round',\n",
       " 'po',\n",
       " 'most',\n",
       " 'did',\n",
       " 'add',\n",
       " 'fore',\n",
       " 'pol',\n",
       " 'again',\n",
       " 'tern',\n",
       " 'know',\n",
       " 'need',\n",
       " 'want',\n",
       " 'see',\n",
       " 'even',\n",
       " 'these',\n",
       " 'use',\n",
       " 'because',\n",
       " 'now',\n",
       " 'make',\n",
       " 'then',\n",
       " 'ower',\n",
       " 'every',\n",
       " 'sec',\n",
       " 'em',\n",
       " 'rit',\n",
       " 'look',\n",
       " 'man',\n",
       " 'row',\n",
       " 'bu',\n",
       " 'where',\n",
       " 'should',\n",
       " 'day',\n",
       " 'rib',\n",
       " 'rel',\n",
       " 'right',\n",
       " 'hen',\n",
       " 'through',\n",
       " 'way',\n",
       " 'don',\n",
       " 'ass',\n",
       " 'reg',\n",
       " 'very',\n",
       " 'other',\n",
       " 'imp',\n",
       " 'sub',\n",
       " 'being',\n",
       " 'does',\n",
       " 'ram',\n",
       " 'down',\n",
       " 'many',\n",
       " 'call',\n",
       " 'before',\n",
       " 'well',\n",
       " 'much',\n",
       " 'those',\n",
       " 'such',\n",
       " 'end',\n",
       " 'ting',\n",
       " 'long',\n",
       " 'think',\n",
       " 'bel',\n",
       " 'its',\n",
       " 'ax',\n",
       " 'own',\n",
       " 'set',\n",
       " 'ife',\n",
       " 'ward',\n",
       " 'show',\n",
       " 'say',\n",
       " 'cess',\n",
       " 'good',\n",
       " 'start',\n",
       " 'made',\n",
       " 'stem',\n",
       " 'up',\n",
       " 'ump',\n",
       " 'mon',\n",
       " 'last',\n",
       " 'stud',\n",
       " 'self',\n",
       " 'min',\n",
       " 'col',\n",
       " 'io',\n",
       " 'count',\n",
       " 'fin',\n",
       " 'air',\n",
       " 'read',\n",
       " 'ever',\n",
       " 'point',\n",
       " 'sur',\n",
       " 'alk',\n",
       " 'used',\n",
       " 'ween',\n",
       " 'same',\n",
       " 'Al',\n",
       " 'while',\n",
       " 'game',\n",
       " 'inter',\n",
       " 'report',\n",
       " 'still',\n",
       " 'led',\n",
       " 'ah',\n",
       " 'here',\n",
       " 'world',\n",
       " 'though',\n",
       " 'arch',\n",
       " 'ale',\n",
       " 'ret',\n",
       " 'ref',\n",
       " 'take',\n",
       " 'way',\n",
       " 'ave',\n",
       " 'going',\n",
       " 'ug',\n",
       " 'spec',\n",
       " 'hand',\n",
       " 'between',\n",
       " 'ear',\n",
       " 'against',\n",
       " 'high',\n",
       " 'gan',\n",
       " 'help',\n",
       " 'ism',\n",
       " 'found',\n",
       " 'land',\n",
       " 'person',\n",
       " 'great',\n",
       " 'sign',\n",
       " 'ser',\n",
       " 'hip',\n",
       " 'run',\n",
       " 'follow',\n",
       " 'find',\n",
       " 'mem',\n",
       " 'ex',\n",
       " 'ense',\n",
       " 'team',\n",
       " 'ash',\n",
       " 'system',\n",
       " 'As',\n",
       " 'min',\n",
       " 'lead',\n",
       " 'cent',\n",
       " 'around',\n",
       " 'govern',\n",
       " 'cur',\n",
       " 'any',\n",
       " 'car',\n",
       " 'ode',\n",
       " 'law',\n",
       " 'read',\n",
       " 'con',\n",
       " 'real',\n",
       " 'support',\n",
       " 'really',\n",
       " 'ness',\n",
       " 'fact',\n",
       " 'day',\n",
       " 'both',\n",
       " 'For',\n",
       " 'three',\n",
       " 'ton',\n",
       " 'each',\n",
       " 'che',\n",
       " 'rep',\n",
       " 'get',\n",
       " 'met',\n",
       " 'ox',\n",
       " 'during',\n",
       " 'ared',\n",
       " 'fam',\n",
       " 'char',\n",
       " 'vent',\n",
       " 'rent',\n",
       " 'formation',\n",
       " 'cor',\n",
       " 'too',\n",
       " 'Z',\n",
       " 'the',\n",
       " 'public',\n",
       " 'prog',\n",
       " 'war',\n",
       " 'power',\n",
       " 'view',\n",
       " 'few',\n",
       " 'different',\n",
       " 'state',\n",
       " 'head',\n",
       " 'poss',\n",
       " 'ret',\n",
       " 'another',\n",
       " 'Q',\n",
       " 'thing',\n",
       " 'child',\n",
       " 'since',\n",
       " 'less',\n",
       " 'life',\n",
       " 'develop',\n",
       " 'pass',\n",
       " 'turn',\n",
       " 'ross',\n",
       " 'resp',\n",
       " 'second',\n",
       " 'oh',\n",
       " 'disc',\n",
       " 'something',\n",
       " 'month',\n",
       " 'government',\n",
       " 'without',\n",
       " 'leg',\n",
       " 'put',\n",
       " 'quest',\n",
       " 'ann',\n",
       " 'never',\n",
       " 'level',\n",
       " 'art',\n",
       " 'might',\n",
       " 'effect',\n",
       " 'cent',\n",
       " 'allow',\n",
       " 'belie',\n",
       " 'feel',\n",
       " 'result',\n",
       " 'lot',\n",
       " 'fun',\n",
       " 'big',\n",
       " 'ask',\n",
       " 'best',\n",
       " 'number',\n",
       " 'lease',\n",
       " 'ca',\n",
       " 'must',\n",
       " 'direct',\n",
       " 'open',\n",
       " 'post',\n",
       " 'come',\n",
       " 'seem',\n",
       " 'week',\n",
       " 'el',\n",
       " 'far',\n",
       " 'tra',\n",
       " 'place',\n",
       " 'form',\n",
       " 'told',\n",
       " 'stand',\n",
       " 'next',\n",
       " 'soc',\n",
       " 'pur',\n",
       " 'let',\n",
       " 'little',\n",
       " 'hum',\n",
       " 'i',\n",
       " 'mark',\n",
       " 'information',\n",
       " 'ways',\n",
       " 'bus',\n",
       " 'invest',\n",
       " 'me',\n",
       " 'hard',\n",
       " 'import',\n",
       " 'test',\n",
       " 'tri',\n",
       " 'rest',\n",
       " 'full',\n",
       " 'care',\n",
       " 'case',\n",
       " 'less',\n",
       " 'ably',\n",
       " 'be',\n",
       " 'list',\n",
       " 'top',\n",
       " 'ration',\n",
       " 'ling',\n",
       " 'reen',\n",
       " 'ger',\n",
       " 'home',\n",
       " 'left',\n",
       " 'better',\n",
       " 'data',\n",
       " 'attack',\n",
       " 'line',\n",
       " 'build',\n",
       " 'aim',\n",
       " 'main',\n",
       " 'got',\n",
       " 'interest',\n",
       " 'keep',\n",
       " 'X',\n",
       " 'class',\n",
       " 'No',\n",
       " 'small',\n",
       " 'ample',\n",
       " 'ide',\n",
       " 'plan',\n",
       " 'percent',\n",
       " 'camp',\n",
       " 'pay',\n",
       " 'ploy',\n",
       " 'Ind',\n",
       " 'els',\n",
       " 'process',\n",
       " 'program',\n",
       " 'ology',\n",
       " 'atter',\n",
       " 'name',\n",
       " 'four',\n",
       " 'return',\n",
       " 'move',\n",
       " 'group',\n",
       " 'men',\n",
       " 'cap',\n",
       " 'ten',\n",
       " 'leg',\n",
       " 'here',\n",
       " 'pat',\n",
       " 'current',\n",
       " 'ides',\n",
       " 'pop',\n",
       " 'to',\n",
       " 'always',\n",
       " 'mil',\n",
       " 'old',\n",
       " 'near',\n",
       " 'ream',\n",
       " 'sh',\n",
       " 'free',\n",
       " 'stand',\n",
       " 'used',\n",
       " 'design',\n",
       " 'change',\n",
       " 'chang',\n",
       " 'bo',\n",
       " 'vis',\n",
       " 'ember',\n",
       " 'book',\n",
       " 'ready',\n",
       " 'kill',\n",
       " 'away',\n",
       " 'able',\n",
       " 'country',\n",
       " 'arn',\n",
       " 'order',\n",
       " 'million',\n",
       " 'ted',\n",
       " 'thing',\n",
       " 'why',\n",
       " 'ural',\n",
       " 'school',\n",
       " 'by',\n",
       " 'Mar',\n",
       " 'days',\n",
       " 'ann',\n",
       " 'ush',\n",
       " 'prof',\n",
       " 'health',\n",
       " 'sol',\n",
       " 'already',\n",
       " 'friend',\n",
       " 'least',\n",
       " 'hist',\n",
       " 'ither',\n",
       " 'son',\n",
       " 'tell',\n",
       " 'talk',\n",
       " 'oint',\n",
       " 'lection',\n",
       " 'until',\n",
       " 'augh',\n",
       " 'later',\n",
       " 'view',\n",
       " 'ending',\n",
       " 'word',\n",
       " 'ware',\n",
       " 'cost',\n",
       " 'enough',\n",
       " 'give',\n",
       " 'arent',\n",
       " 'par',\n",
       " 'rist',\n",
       " 'large',\n",
       " 'side',\n",
       " 'win',\n",
       " 'important',\n",
       " 'business',\n",
       " 'clear',\n",
       " 'aster',\n",
       " 'alf',\n",
       " 'American',\n",
       " 'expect',\n",
       " 'kind',\n",
       " 'mean',\n",
       " 'past',\n",
       " 'dev',\n",
       " 'bas',\n",
       " 'let',\n",
       " 'raft',\n",
       " 'organ',\n",
       " 'perform',\n",
       " 'story',\n",
       " 'season',\n",
       " 'claim',\n",
       " 'came',\n",
       " 'within',\n",
       " 'line',\n",
       " 'project',\n",
       " 'control',\n",
       " 'ended',\n",
       " 'air',\n",
       " 'ley',\n",
       " 'money',\n",
       " 'for',\n",
       " 'family',\n",
       " 'making',\n",
       " 'bit',\n",
       " 'police',\n",
       " 'happen',\n",
       " 'sit',\n",
       " 'sure',\n",
       " 'gin',\n",
       " 'appear',\n",
       " 'light',\n",
       " 'es',\n",
       " 'of',\n",
       " 'water',\n",
       " 'times',\n",
       " 'not',\n",
       " 'grow',\n",
       " 'company',\n",
       " 'mar',\n",
       " 'arm',\n",
       " 'example',\n",
       " 'fore',\n",
       " 'pro',\n",
       " 'actually',\n",
       " 'ever',\n",
       " 'tax',\n",
       " 'major',\n",
       " 'ama',\n",
       " 'often',\n",
       " 'eral',\n",
       " 'human',\n",
       " 'job',\n",
       " 'available',\n",
       " 'aid',\n",
       " 'record',\n",
       " 'sing',\n",
       " 'news',\n",
       " 'following',\n",
       " 'hour',\n",
       " 'most',\n",
       " 'sex',\n",
       " 'become',\n",
       " 'Ed',\n",
       " 'took',\n",
       " 'product',\n",
       " 'As',\n",
       " 'hop',\n",
       " 'cho',\n",
       " 'certain',\n",
       " 'non',\n",
       " 'deal',\n",
       " 'side',\n",
       " 'May',\n",
       " 'reason',\n",
       " 'elect',\n",
       " 'official',\n",
       " 'possible',\n",
       " 'hold',\n",
       " 'city',\n",
       " 'sever',\n",
       " 'once',\n",
       " 'night',\n",
       " 'John',\n",
       " 'ape',\n",
       " 'play',\n",
       " 'done',\n",
       " 'lim',\n",
       " 'working',\n",
       " 'body',\n",
       " 'Mr',\n",
       " 'whether',\n",
       " 'author',\n",
       " 'proper',\n",
       " 'seen',\n",
       " 'cond',\n",
       " 'course',\n",
       " 'event',\n",
       " 'pot',\n",
       " 'intern',\n",
       " 'short',\n",
       " 'empt',\n",
       " 'God',\n",
       " 'ability',\n",
       " 'dam',\n",
       " 'press',\n",
       " 'doing',\n",
       " 'protect',\n",
       " 'ring',\n",
       " 'thought',\n",
       " 'question',\n",
       " 'several',\n",
       " 'State',\n",
       " 'given',\n",
       " 'fund',\n",
       " 'went',\n",
       " 'work',\n",
       " 'my',\n",
       " 'meet',\n",
       " 'creat',\n",
       " 'raw',\n",
       " 'understand',\n",
       " 'lish',\n",
       " 'won',\n",
       " 'agon',\n",
       " 'love',\n",
       " 'complete',\n",
       " 'par',\n",
       " 'account',\n",
       " 'vert',\n",
       " 'prob',\n",
       " 'young',\n",
       " 'along',\n",
       " 'according',\n",
       " 'yet',\n",
       " 'among',\n",
       " 'ai',\n",
       " 'employ',\n",
       " 'low',\n",
       " 'likely',\n",
       " 'ant',\n",
       " 'aged',\n",
       " 'Russ',\n",
       " 'ben',\n",
       " 'For',\n",
       " 'back',\n",
       " 'president',\n",
       " 'ball',\n",
       " 'access',\n",
       " 'known',\n",
       " 'early',\n",
       " 'use',\n",
       " 'fight',\n",
       " 'sent',\n",
       " 'today',\n",
       " 'market',\n",
       " 'based',\n",
       " 'strong',\n",
       " 'deb',\n",
       " 'problem',\n",
       " 'death',\n",
       " 'social',\n",
       " 'campaign',\n",
       " 'ey',\n",
       " 'er',\n",
       " 'iron',\n",
       " 'viol',\n",
       " 'five',\n",
       " 'stre',\n",
       " 'year',\n",
       " 'success',\n",
       " 'present',\n",
       " 'particular',\n",
       " 'try',\n",
       " 'suggest',\n",
       " 'Christ',\n",
       " 'land',\n",
       " 'local',\n",
       " 'mult',\n",
       " 'political',\n",
       " 'former',\n",
       " 'close',\n",
       " 'getting',\n",
       " 'across',\n",
       " 'comb',\n",
       " 'believe',\n",
       " 'z',\n",
       " 'together',\n",
       " 'individual',\n",
       " 'face',\n",
       " 'value',\n",
       " 'area',\n",
       " 'writ',\n",
       " 'key',\n",
       " 'put',\n",
       " 'anything',\n",
       " 'experience',\n",
       " 'mind',\n",
       " 'future',\n",
       " 'ged',\n",
       " 'cut',\n",
       " 'tot',\n",
       " 'itch',\n",
       " 'video',\n",
       " 'net',\n",
       " 'though',\n",
       " 'connect',\n",
       " 'resent',\n",
       " 'above',\n",
       " 'amount',\n",
       " 'either',\n",
       " 'anal',\n",
       " 'fail',\n",
       " 'special',\n",
       " 'black',\n",
       " 'looking',\n",
       " 'fire',\n",
       " 'yn',\n",
       " 'almost',\n",
       " 'study',\n",
       " 'miss',\n",
       " 'community',\n",
       " 'media',\n",
       " 'food',\n",
       " 'comes',\n",
       " 'single',\n",
       " 'half',\n",
       " 'ague',\n",
       " 'hod',\n",
       " 'quick',\n",
       " 'book',\n",
       " 'issue',\n",
       " 'else',\n",
       " 'consider',\n",
       " 'taken',\n",
       " 'true',\n",
       " 'wa',\n",
       " 'ago',\n",
       " 'mess',\n",
       " 'added',\n",
       " 'bad',\n",
       " 'similar',\n",
       " 'ask',\n",
       " 'Don',\n",
       " 'character',\n",
       " 'type',\n",
       " 'entire',\n",
       " 'history',\n",
       " 'live',\n",
       " 'trying',\n",
       " 'discuss',\n",
       " 'self',\n",
       " 'rest',\n",
       " 'room',\n",
       " 'elt',\n",
       " 'fall',\n",
       " 'x',\n",
       " 'idea',\n",
       " 'bo',\n",
       " 'sound',\n",
       " 'someone',\n",
       " 'object',\n",
       " 'aper',\n",
       " 'player',\n",
       " 'rather',\n",
       " 'service',\n",
       " 'Part',\n",
       " 'rug',\n",
       " 'mon',\n",
       " 'ply',\n",
       " 'mor',\n",
       " 'nothing',\n",
       " 'provide',\n",
       " 'party',\n",
       " 'exist',\n",
       " 'mag',\n",
       " 'house',\n",
       " 'behind',\n",
       " 'however',\n",
       " 'sum',\n",
       " 'function',\n",
       " 'front',\n",
       " 'series',\n",
       " 'opt',\n",
       " 'below',\n",
       " 'specific',\n",
       " 'ra',\n",
       " 'previous',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0591c4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Ġthat'.replace('Ġ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a0d32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n#ltk.pos_tag('that')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8959e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgagne/.conda/envs/env_jupyter_for_scann/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "285fa4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "052d751f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADV'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('in')[0].pos_\n",
    "nlp('over')[0].pos_\n",
    "nlp('overly')[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6ffa8975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Ġcat', 'Ġjumps']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('He was overly excited')\n",
    "tokenizer.tokenize('The cat jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f6a160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "verbs = []\n",
    "prons = []\n",
    "for word in whole_words[0:1000]:\n",
    "    if len(word)>3:\n",
    "        pos = nlp(word)[0].pos_\n",
    "        if pos=='NOUN':\n",
    "            nouns.append(word)\n",
    "        if pos=='VERB':\n",
    "            verbs.append(word)\n",
    "        if pos=='PRON':\n",
    "            prons.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34912664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that',\n",
       " 'this',\n",
       " 'they',\n",
       " 'their',\n",
       " 'your',\n",
       " 'which',\n",
       " 'them',\n",
       " 'some',\n",
       " 'what',\n",
       " 'these']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prons[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76bf512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'said',\n",
       " 'ates',\n",
       " 'hing',\n",
       " 'know',\n",
       " 'want',\n",
       " 'make',\n",
       " 'look',\n",
       " 'does',\n",
       " 'call']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "22be0844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ally',\n",
       " 'year',\n",
       " 'port',\n",
       " 'part',\n",
       " 'time',\n",
       " 'people',\n",
       " 'work',\n",
       " 'form',\n",
       " 'play',\n",
       " 'fore']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "797f90be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if 'D' in nlp.vocab:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "284c1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "another give them camp.\n",
      "some deal anything side.\n",
      "every provide each viol.\n",
      "anything oint your side.\n",
      "their give every question.\n",
      "your seen some ability.\n",
      "every follow those group.\n",
      "anything start their fire.\n",
      "these employ those word.\n",
      "some lish what country.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentence= '{PRON1} {VERB} {PRON2} {NOUN}.'\n",
    "    sentence= sentence.replace('{PRON1}',np.random.choice(prons))\n",
    "    sentence= sentence.replace('{PRON2}',np.random.choice(prons))\n",
    "    sentence= sentence.replace('{VERB}',np.random.choice(verbs))\n",
    "    sentence= sentence.replace('{NOUN}',np.random.choice(nouns))\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "975bda01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_words.index('give')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "77fe388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allowed_toks = tokenizer(['The', 'A', \n",
    "                          'dog', 'cat', 'man',\n",
    "                          'ran','ate','walked', 'thought',\n",
    "                          'to','under','the', 'a','about','through',\n",
    "                         'tree','river','fire','death','horrible','great'],\n",
    "            add_prefix_space=True).input_ids\n",
    "allowed_toks.extend(tokenizer(['The', 'A'],\n",
    "            add_prefix_space=False).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fcadc1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(allowed_toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e7deea8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[383],\n",
       " [317],\n",
       " [3290],\n",
       " [3797],\n",
       " [582],\n",
       " [4966],\n",
       " [15063],\n",
       " [6807],\n",
       " [1807],\n",
       " [284],\n",
       " [739],\n",
       " [262],\n",
       " [257],\n",
       " [546],\n",
       " [832],\n",
       " [5509],\n",
       " [7850],\n",
       " [2046],\n",
       " [1918],\n",
       " [12361],\n",
       " [1049],\n",
       " [464],\n",
       " [32]]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "93974a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The [383]\n",
      " A [317]\n",
      " dog [3290]\n",
      " cat [3797]\n",
      " man [582]\n",
      " ran [4966]\n",
      " ate [15063]\n",
      " walked [6807]\n",
      " thought [1807]\n",
      " to [284]\n",
      " under [739]\n",
      " the [262]\n",
      " a [257]\n",
      " about [546]\n",
      " through [832]\n",
      " tree [5509]\n",
      " river [7850]\n",
      " fire [2046]\n",
      " death [1918]\n",
      " horrible [12361]\n",
      " great [1049]\n",
      "The [464]\n",
      "A [32]\n"
     ]
    }
   ],
   "source": [
    "for tok in allowed_toks:\n",
    "    print(tokenizer.decode(tok), tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db865a4",
   "metadata": {},
   "source": [
    "### Top english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "aeabd18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word\n",
       "1       the\n",
       "2        be\n",
       "3       and\n",
       "4         a\n",
       "5        of\n",
       "..      ...\n",
       "994    date\n",
       "995  public\n",
       "996    army\n",
       "997     top\n",
       "998    post\n",
       "\n",
       "[998 rows x 1 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_english_words = pd.read_csv('../mscl/top_english_words.tsv',\n",
    "                                sep='\\t',header=None,index_col=0, names=['word'])\n",
    "top_english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "55944896",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in top_english_words.word:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    top_english_words.loc[top_english_words.word==word,'tokens']=''.join(tokens)\n",
    "    top_english_words.loc[top_english_words.word==word,'n_tokens']=len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "67e549a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_single_tok_english_words = top_english_words[top_english_words.n_tokens==1].word.values\n",
    "np.save('../mscl/top_single_tok_english_words.npy', top_single_tok_english_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cad429",
   "metadata": {},
   "source": [
    "### Top words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2b31758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = '../data/raw/social_i_qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7379793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(dataset)\n",
    "dataset = dataset.rename_column('context','text')\n",
    "dataset = dataset.remove_columns(['question', 'answerA', 'answerB', 'answerC', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "32ae804a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 281, 5128]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.build_inputs_with_special_tokens('This is an input')\n",
    "tokenizer.encode('This is an input', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4cac9b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cameron decided to have a barbecue and gathered her friends together.',\n",
       " 'Jan needed to give out jobs for an upcoming project at work.',\n",
       " \"Remy was an expert fisherman and was on the water with Kai. Remy baited Kai's hook.\",\n",
       " \"Addison gave a hug to Skylar's son when they were feeling down.\",\n",
       " 'Kai found one for sale online but it was too much money for her.',\n",
       " \"Quinn was high up in the tree and didn't want to get down. We told him we would play a game, so Quinn came down to us.\",\n",
       " \"Bailey found Carson's cleaning solution after not being able to find it by himself.\",\n",
       " 'Kendall worked the weekend at the steakhouse and made bank on tips.',\n",
       " 'Quinn wanted to help me clean my room up because it was so messy.',\n",
       " 'Kendall ran back and thanked Lee for helping her find the dog.',\n",
       " 'Sydney was a school teacher and made sure their students learned well.',\n",
       " \"Sasha's mom passed out in the middle of the party. Aubrey took Sasha's mom to the hospital.\",\n",
       " \"Cameron promised not to tell anyone about Ash's surprise party. Cameron told someone the secret.\",\n",
       " 'Their cat kept trying to escape out of the window, so Jan placed an obstacle in the way.',\n",
       " \"Remy blinked Quinn's eyes because Quinn was paralyzed after the car accident.\",\n",
       " 'Austin saw that someone left their purse on top of their car as they drove away so he ran along side them to get their attention.',\n",
       " 'Jordan sliced open the dead body to harvest the organs.',\n",
       " 'Ash rode hard and put away his wet clothes to let them dry for a few hours.',\n",
       " 'Bailey relieved every one of her friends when she announced her plans to stay.',\n",
       " 'Sydney had so much pent up emotion, they burst into tears at work.',\n",
       " 'Bailey took Taylor by surprise when she showed up on the same cruise.',\n",
       " 'Sydney got a raise and a new promotion.',\n",
       " 'Quinn finally found their lost puppy that ran away last week.',\n",
       " \"skylar was hanging out with kendall so she met kendall's boyfriend's parents.\",\n",
       " 'Remy enjoyed guiding and helping her friends through their problems.',\n",
       " 'Remy was athletic and wanted to spend time in nature, so Remy wanted to go hiking.',\n",
       " 'The teacher asked the class a question and they seemed puzzled. Aubrey understood the question well and answered.',\n",
       " 'Aubrey succeeded at what they were trying to do and felt like a success.',\n",
       " 'Kendall frightened the dogs away by yelling and waving their hands.',\n",
       " 'Jan had always wanted a puppy, but decided to adopt an older shelter dog instead.',\n",
       " 'Addison bid Carson goodbye before the ship left the dock.',\n",
       " 'After Sydney cut her nails, Sasha decided to get her hair done and go on a date.',\n",
       " 'Cameron flew on a plane because he figured he would get there faster than driving.',\n",
       " 'Jan bound together the ropes tightly and gave them one final tug to ensure the boat was secured.',\n",
       " 'Riley told Taylor that they had found a dog tied up in a park.',\n",
       " 'Carson was very passionate about this topic so he drove home his point.',\n",
       " 'Skylar returned early in the evening after a night and day of partying.',\n",
       " 'It was late and night and Bailey was asleep in bed alone.',\n",
       " 'Carson made lots of friends when they went to the club last night.',\n",
       " 'Tracy protected her teammates from injury when she saw an accident about to happen prevented it.',\n",
       " 'Quinn was about to die when the doctor provided professional treatment.',\n",
       " \"During their yearbook class, Alex took an old photo of herself and Quin, and crossed out Quinn's face on the photograph. Quinn saw her do this.\",\n",
       " 'The customer felt that Addison was rude and made a complaint to management.',\n",
       " 'Aubrey was working the door at a concert, so Aubrey asked the girl coming in for money.',\n",
       " 'casey was in a rush to see what was in the envelope so he tore open the letter.',\n",
       " \"Tracy was teaching Kendall to dance so Tracy raised Kendall's hips into the right position for the next move.\",\n",
       " 'Remy gave Robin the feeling of love by having sex with her in bed.',\n",
       " 'Robin had watched their partner Alex perform in a play and had loved it.',\n",
       " 'Carson and Addison are dating and it is getting serious day by day.',\n",
       " 'Skylar applied their desires unto Sasha because she was demanding and bossy.',\n",
       " 'Bailey took their dog to the dog park and let the dog run.',\n",
       " 'Sasha loved animals and was bringing them to a new sanctuary where they were safe.',\n",
       " 'Lee was spending too much money on repairs so he wanted to save as much as possible by shopping around.',\n",
       " 'kendall was hungry for the large hot dog so she took another bite.',\n",
       " \"Taylor saved other people's teeth from decay by providing good toothpaste.\",\n",
       " 'The boxes were in the doorway, so Alex moved the boxes from the doorway.',\n",
       " 'Austin fought for Quinn and she was very happy to know him.',\n",
       " 'Kai taught English class to students at a college.',\n",
       " \"Jordan ate all of Sydney's food and had to get more because Sydney was hungry.\",\n",
       " 'Addison had no clue what to do after hearing the news so Addison wanted to go to Ash for advice.',\n",
       " 'Ash wanted to see Carson again after they spent a weekend playing games.',\n",
       " 'Carson took the day off work after he felt sick to his stomach and threw up.',\n",
       " 'QUiin looked for hours and in the end finally found the item.',\n",
       " \"Addison looked over bailey's business plan and helped her maximize her profits.\",\n",
       " 'Ash was worried how it would go, but according to Ash it went well.',\n",
       " 'Jesse lost every penny gambling on sports.',\n",
       " 'Taylor used a chart to help explaining the complex logic and design of the system to the teammates.',\n",
       " 'Kendall wanted to ask Jan if she had gotten a date to the dance yet.',\n",
       " 'Sydney played basketball with her friends after school on a sunny afternoon.',\n",
       " 'At the bar after work while they were having a drink, Carson got angry at Taylor.',\n",
       " \"Riley was working on a project and Jan made Riley's object.\",\n",
       " 'It was getting time and Sasha got ready to go to the party.',\n",
       " 'Sasha wanted to lose some weight before their cheer competition.',\n",
       " 'When pass the test after studying all night with no sleep.',\n",
       " 'Remy posted a picture on her social media and hoped to get a lot of upvotes.',\n",
       " 'In order to remember the good times past. Cameron always put up pictures.',\n",
       " 'Jordan had a handful of lottery tickets, and gave Taylor the right one to win the jackpot.',\n",
       " 'After learning that Cameron was trying to have an affair with their husband, Quinn tried to kill Cameron.',\n",
       " 'Quinn was a cook at a school. Quinn made sandwiches for others.',\n",
       " 'Since they were the teacher and needed to make things clear, Kendall proved every point.',\n",
       " 'Remy got a new puppy today and taught him how to sit.',\n",
       " 'Quinn was thirsty and drank a lot of milk with his cookies.',\n",
       " \"Jordan told Riley's boyfriend yes to going to the movies with them as a coupl.\",\n",
       " 'Carson wanted to go straight away but their car had low air in the tires.',\n",
       " 'Kendall spends money unwisely, so Robin makes her deposit their money in the bank.',\n",
       " 'Riley reached the point climbing the hill where Kendall could see them over the rise.',\n",
       " 'Skylar became a millionaire through hard work and used the money wisely.',\n",
       " \"Riley was jumping up and down with their hands in the air. Austin held Riley's arms to keep them from moving.\",\n",
       " 'Skylar made amends with his friend after the argument.',\n",
       " 'Quinn had too much to drink the other night. They were starting to have a problem.',\n",
       " 'Aubrey sent text messages to remind the group about the meeting this afternoon.',\n",
       " 'Sydney reduced national debt to a manageable level by stimulating the economy and reducing taxes.',\n",
       " 'Austin stretched out their arms to reach the item at the end of the table.',\n",
       " 'Ash understood another idea. That idea was better than the original.',\n",
       " 'Alex wanted their hair to be bright blue but needed to remove their natural color first.',\n",
       " 'Taylor took the poor dog she found on the road to the vet.',\n",
       " 'jesse was bored so he saw if he could help the person out.',\n",
       " \"Addison allayed their fears by imaging what the worst that could happen. It wasn't so bad.\",\n",
       " \"Aubrey made Kendall's bed because Kendall did not do it.\",\n",
       " 'Skylar had a cousin who had become addicted to methamphetamine. She was worried she would return to old habits after making it through rehab sucessfully.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c9b3e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/mini_data.txt', 'a') as the_file:\n",
    "    for line in dataset['train']['text'][0:100]:\n",
    "        the_file.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a2813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
